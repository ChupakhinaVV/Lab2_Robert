# -*- coding: utf-8 -*-
"""2lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/ChupakhinaVV/912c1d862196a95851c862bc1cb0d951/2lab.ipynb
"""

# 1. Установка библиотек (запускается один раз)
!pip install transformers torch scikit-learn pandas matplotlib tqdm

# 2. Импорты
import pandas as pd
import torch
from transformers import RobertaTokenizer, RobertaModel
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from tqdm import tqdm
import matplotlib.pyplot as plt

# 3. Загрузка и фильтрация датасета
df = pd.read_csv("data/youtube.csv")
df = df.drop_duplicates(subset=["description"])
df = df[df["description"].str.strip().astype(bool)]
df["label"] = df["description"].apply(lambda x: 1 if len(x) >= 300 else 0)

# Балансировка: по 500 примеров с коротким и длинным описанием
short_df = df[df["label"] == 0].sample(500, random_state=42)
long_df = df[df["label"] == 1].sample(500, random_state=42)
df_balanced = pd.concat([short_df, long_df]).reset_index(drop=True)

# 4. Загрузка модели RoBERTa
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaModel.from_pretrained("roberta-base")
model.eval()

# 5. Получение эмбеддингов CLS-токена
embeddings = []
batch_size = 16

with torch.no_grad():
    for i in tqdm(range(0, len(df_balanced), batch_size)):
        texts = df_balanced["description"].iloc[i:i+batch_size].tolist()
        inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
        outputs = model(**inputs)
        cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()
        embeddings.extend(cls_embeddings)

X = torch.tensor(embeddings)
y = torch.tensor(df_balanced["label"].values)

# 6. Обучение модели и оценка
X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=42)

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred, target_names=["short", "long"]))

# 7. Исследование эмбеддингов с разных слоёв трансформера
# В RoBERTa 12 слоёв: получим CLS из 1-го, 4-го, 8-го и последнего

from transformers import RobertaModel

model = RobertaModel.from_pretrained("roberta-base", output_hidden_states=True)
model.eval()

layer_results = {}
layers_to_check = [1, 4, 8, 11]

with torch.no_grad():
    for layer in layers_to_check:
        layer_embeddings = []
        for i in tqdm(range(0, len(df_balanced), batch_size), desc=f"Layer {layer}"):
            texts = df_balanced["description"].iloc[i:i+batch_size].tolist()
            inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
            outputs = model(**inputs)
            cls = outputs.hidden_states[layer][:, 0, :].numpy()
            layer_embeddings.extend(cls)
        # Обучение
        X_layer = torch.tensor(layer_embeddings)
        X_train, X_test, y_train, y_test = train_test_split(X_layer.numpy(), y.numpy(), test_size=0.2, random_state=42)
        clf = LogisticRegression(max_iter=1000)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        report = classification_report(y_test, y_pred, output_dict=True)
        layer_results[f"Layer {layer+1}"] = report["weighted avg"]["f1-score"]

# 8. Визуализация результатов
plt.figure(figsize=(8, 5))
plt.bar(layer_results.keys(), layer_results.values())
plt.title("F1-score по слоям RoBERTa")
plt.ylabel("F1-score")
plt.xlabel("Слой")
plt.grid(True)
plt.show()